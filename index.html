<!DOCTYPE html>
<html>
<head>
  <title>Flow-Retrieval: Flow-Guided Data Retrieval for Few-Shot Imitation Learning</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/style.css"
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .video-container {
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100%;
    }
    .video-container video {
        width: 40%;
        height: 40%;
        object-fit: contain;  /* Maintains aspect ratio and fits within the container */
    }
</style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- TODO: use css to replace span -->
          <h1 class="title is-1 publication-title"><span class="textsc">FlowRetrieval</span>: Flow-Guided Data Retrieval for Few-Shot Imitation Learning</h1>
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
          </div> -->

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <center>
    <video autoplay controls muted loop playsinline>
      <source src="./static/videos/overview.mp4"
              type="video/mp4">
    </video>
  </center>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <p>
        <span class="textsc">FlowRetrieval</span> leverages optical flow representations for <strong>extracting relevant prior data</strong> and <strong>guiding policy learning</strong> to maximally benefit from the retrieved data.
      </p>
    </div>
  </div>
</section>

<!-- Rollouts -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <center>
            <video autoplay controls muted loop playsinline width="56.25%">
              <source src="./static/videos/square_rollout.mp4"
                      type="video/mp4">
            </video>
          </center>
        </div>
        <div class="item">
          <center>
            <video autoplay controls muted loop playsinline width="56.25%">
              <source src="./static/videos/libero-can_rollout.mp4"
                      type="video/mp4">
            </video>
          </center>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/bridge-pot_rollout.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/bridge-microwave_rollout.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/franka-pen-in-cup_rollout.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <p>
          Our method uses only <strong>10 target task demonstrations</strong> to learn the policy.
        </p>
      </div>
    </div> 
  </div>
</section>
<!--/ Rollouts. --> 

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Imitation learning in robotics requires extensive amount of demonstrations while zero-shot performance of such pretrained policies often struggle.
            It is critical to develop few-shot adaptation strategies that rely on a small amount of demonstrations for the target task.
            Recent research has shown that augmenting training data with past experiences can provide abundant signals when learning from small data.
            However, existing data retrieval methods fall under two extremes: they either rely on the existence of <em>exact same</em> behaviors with visually similar objects in the prior data, which is impractical to assume; or they retrieve based on semantic similarity of high-level language descriptions of the task, which might not be that informative about the shared behaviors or motions across tasks.
            In this work, we investigate how we can leverage the vast amount of cross-task data that share similar <em>motion</em> with the target task to improve few-shot imitation learning.
            Our key insight is that motion-similar data carry rich information about the effects of actions and object interactions that can be leveraged during few-shot adaptation.
            We propose <span class="textsc">FlowRetrieval</span>, an approach that leverages optical flow representations for both extracting similar motions to target tasks from prior data, and for guiding learning of a policy that can maximally benefit from such data.
            Our results show <span class="textsc">FlowRetrieval</span> significantly outperforms prior methods across simulated and real-world domains, with over 4X performance of vanilla imitation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<!-- Method Overview. -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
          <div class="content">
            <h2 class="title is-3">Method Overview</h2>
            <p>
              We learn a motion-centric latent space for retrieving motions similar to target data from prior data and further guide policy learning with optical flow.
            </p>
            <img src="./static/images/1_overview.png"
                  class="method-overview-image"
                  alt="Method Overview"/>
            <ul>
              <li><strong>Motion-Centric Pretraining</strong>: <span class="textsc">FlowRetrieval</span> acquires a motion-centric latent space by computing optical flow between the current frame and a future frame of the robot's RGB visual observations, and employing a variational autoencoder (VAE) to embed the optical flow data.</li>
              <li><strong>Data Retrieval with the Learned Motion-Centric Latent Space</strong>: We select the nearest neighbors of target task in the latent space from previously collected data.</li>
              <li><strong>Flow-Guided Learning</strong>: During policy learning, <span class="textsc">FlowRetrieval</span> leverages an auxiliary loss of predicting the optical flow as additional guidance for representation learning, encouraging the model to encode the image with enough details to reconstruct optical flow alongside predicting the action.</li>
            </ul>
          </div>
    </div>
  </div>
</section>
<!--/ Method Overview. -->

<!-- Experiments. -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experiments</h2>

    <h3 class="title is-4">Tasks</h3>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h4 class="title is-6">Square Assembly (Target | Bad Prior | Good Prior)</h4>
          <img src="./static/videos/square_dataset-visualization.gif"/>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <h4 class="title is-6">LIBERO-Can (Target | Prior 1 | Prior 2)</h4>
          <img src="./static/videos/libero-can_dataset-visualization.gif"/>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h4 class="title is-6">Bridge-Pot (Target | Prior 1 | Prior 2)</h4>
          <img src="./static/videos/bridge-pot_dataset-visualization.gif"/>
        </div>
      </div>

      <div class="column">
        <div class="content">
          <h4 class="title is-6">Bridge-Microwave (Target | Prior 1 | Prior 2)</h4>
          <img src="./static/videos/bridge-microwave_dataset-visualization.gif"/>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-half">
        <div class="content">
          <h4 class="title is-6 has-text-centered">Franka-Pen-in-Cup (Target | PnP | Wild)</h4>
          <img src="./static/videos/franka-pen-in-cup_dataset-visualization.gif"/>
        </div>
      </div>
    </div>
    <br/>

    <h3 class="title is-4">Quantitative Results</h3>
    <p style="text-align:center;"><img src="./static/images/5_results_full.png"/></p>
    <div class="content">
      <ul>
        <li><span class="textsc">FlowRetrieval</span> outperforms the best baseline method across different tasks, achieving an average of 14% higher success rate than the best baseline method in each domain (+10% in simulation,+19% in real).</li>
        <li><span class="textsc">FlowRetrieval</span> also achieves on average 27% higher success rate than the best retrieval-based prior method.</li>
      </ul>
    </div>
    <br/>

    <h3 class="title is-4">Qualitative Analysis</h3>
    <p style="text-align:center;"><img src="./static/images/5_retrieval_viz_old.png"/></p>
    <div class="content">
      <p><span class="textsc">FlowRetrieval</span> retrieves similar motion from prior data while <strong>BR</strong> and <strong>SR</strong> retrsieve based on visual similarity of the state and consequently can retrieve adversarial data.</p>
      <ul>
        <li>In the <em>Square Assembly</em> example, both baselines end up retrieving data points where the robot moves towards the round peg. <span class="textsc">FlowRetrieval</span> on the other hand selects a very similar motion toward the square peg even when the background color is completely different from that in target data.</li>
        <li>In <em>LIBERO-Can</em>, <strong>BR</strong> and <strong>SR</strong> again focus mainly on visual similarity instead of the picking up motion and retrieved data points that have very different motion (moving downwards). However, <span class="textsc">FlowRetrieval</span> retrieves the correct motion of picking up, even if it is picking up a different object than in the target task.</li>
      </ul>
    </div>
    </p>
    <br/>
    <p style="text-align:center;"><img src="./static/images/app_franka.png"/></p>
    <div class="content">
      <ul>
        <li>When retrieving from the <em>PnP</em> dataset, <span class="textsc">FlowRetrieval</span> focuses on the pick-up and transfer stage of the target task, and does not retrieve the placing motions from the prior dataset, effectively filtering adversarial prior data.</li>
        <li>When retrieving from the <em>Wild</em> dataset, we see that <span class="textsc">FlowRetrieval</span> retrieves viewpoints better aligned with that in the target task, while ProprioRetrieval retrieves very different viewpoints (sometimes the robot is not even in the view -- see example in rightmost column, second from bottom).</li>
      </ul>
    </div>
    <br/>
    <p style="text-align:center;"><img src="./static/images/5_square_retrieval_bar.png"/></p>
    <p>
      In the <em>Square Assembly</em> task, the data right after the robot picks up the nut is crucial: actions that move towards the wrong goal are adversarial data. The data points after the forking point, while possibly performing a different task, are considered non-harmful.
      We can therefore analyze the quality of retrieved data under different retrieval methods by plotting the amount of different types of data retrieved from each stage of the task (split into 10 bins).
      <span class="textsc">FlowRetrieval</span> uniformly retrieves from prior useful data and retrieves little adversarial data, while baseline models either cannot effectively filter out the adversarial data or does not retrieve enough useful data at the bottleneck stage (between <em>pick up</em> and <em>transfer</em>) of the task.
    </p>

  </div>
</section>
<!--/ Experiments. --> 

<!-- 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template was borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>